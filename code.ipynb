{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # 定义逻辑斯蒂克（logistics）目标函数\n",
    "# def logistics_function(X, y, weights):\n",
    "#     z = np.dot(X, weights)\n",
    "#     predictions = 1 / (1 + np.exp(-z))\n",
    "#     loss = -np.mean(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))\n",
    "#     return loss\n",
    "\n",
    "# # 计算逻辑斯蒂克（logistics）目标函数关于参数的梯度\n",
    "# def gradient(X, y, weights):\n",
    "#     z = np.dot(X, weights)\n",
    "#     predictions = 1 / (1 + np.exp(-z))\n",
    "#     error = predictions - y\n",
    "#     grad = np.dot(X.T, error) / len(X)  #logit损失函数对模型形式参数的偏导\n",
    "#     return grad\n",
    "\n",
    "# # 小批量梯度下降算法\n",
    "# def mini_batch_gradient_descent(X, y, learning_rate=0.01, lamda =0.01,batch_size=32, num_epochs=100):\n",
    "#     # 初始化参数\n",
    "#     weights = np.random.randn(X.shape[1])\n",
    "    \n",
    "#     # 迭代训练\n",
    "#     for epoch in range(num_epochs):\n",
    "#         # 在每个epoch开始时，对训练数据进行洗牌\n",
    "#         shuffled_indices = np.random.permutation(len(X))\n",
    "#         X_shuffled = X[shuffled_indices]\n",
    "#         y_shuffled = y[shuffled_indices]\n",
    "        \n",
    "#         # 按批次更新参数\n",
    "#         for batch_start in range(0, len(X), batch_size):\n",
    "#             # 获取当前批次的数据\n",
    "#             X_batch = X_shuffled[batch_start:batch_start+batch_size]\n",
    "#             y_batch = y_shuffled[batch_start:batch_start+batch_size]\n",
    "            \n",
    "#             # 计算梯度\n",
    "#             grad = gradient(X_batch, y_batch, weights)\n",
    "            \n",
    "#             # 更新参数\n",
    "#             batch_weights = weights - learning_rate * grad\n",
    "#             weights = weights - learning_rate * grad +lamda*\n",
    "            \n",
    "#         # 计算当前参数下的目标函数值\n",
    "#         cost = logistics_function(X, y, weights)\n",
    "        \n",
    "#         # 输出损失\n",
    "#         print(\"Epoch: {}, Cost: {}\".format(epoch, cost))\n",
    "\n",
    "# # # 创建训练数据（示例数据，自行替换）\n",
    "# # np.random.seed(0)\n",
    "# # X = np.random.randn(1000, 3)\n",
    "# # y = np.random.randint(2, size=1000)\n",
    "\n",
    "# # # 添加偏置项\n",
    "# # X_bias = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "# # 运行小批量梯度下降算法\n",
    "# mini_batch_gradient_descent(X_bias, y, learning_rate=0.01, batch_size=32, num_epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cost: 1.233526075497843\n",
      "Epoch: 1, Cost: 1.0608433123427659\n",
      "Epoch: 2, Cost: 0.9541114893203407\n",
      "Epoch: 3, Cost: 0.897778038022909\n",
      "Epoch: 4, Cost: 0.866898521332361\n",
      "Epoch: 5, Cost: 0.8509932657306752\n",
      "Epoch: 6, Cost: 0.8424559757886418\n",
      "Epoch: 7, Cost: 0.8373311350558046\n",
      "Epoch: 8, Cost: 0.834200555933608\n",
      "Epoch: 9, Cost: 0.8320761922726225\n",
      "Epoch: 10, Cost: 0.8304482829865532\n",
      "Epoch: 11, Cost: 0.8290515874783061\n",
      "Epoch: 12, Cost: 0.8276815580970582\n",
      "Epoch: 13, Cost: 0.826408775357709\n",
      "Epoch: 14, Cost: 0.8252077049469461\n",
      "Epoch: 15, Cost: 0.8240745649422625\n",
      "Epoch: 16, Cost: 0.822900270936883\n",
      "Epoch: 17, Cost: 0.8216858004255626\n",
      "Epoch: 18, Cost: 0.8205352887211343\n",
      "Epoch: 19, Cost: 0.8194228181827308\n",
      "Epoch: 20, Cost: 0.8182720142455631\n",
      "Epoch: 21, Cost: 0.8171649946182411\n",
      "Epoch: 22, Cost: 0.8160352373956813\n",
      "Epoch: 23, Cost: 0.8148661615383103\n",
      "Epoch: 24, Cost: 0.8137493827084618\n",
      "Epoch: 25, Cost: 0.8126227244659215\n",
      "Epoch: 26, Cost: 0.8114800659596565\n",
      "Epoch: 27, Cost: 0.8103904693699239\n",
      "Epoch: 28, Cost: 0.809297810205022\n",
      "Epoch: 29, Cost: 0.8082067254667329\n",
      "Epoch: 30, Cost: 0.807191931012473\n",
      "Epoch: 31, Cost: 0.8061793897481189\n",
      "Epoch: 32, Cost: 0.8050684635961675\n",
      "Epoch: 33, Cost: 0.8040508091720585\n",
      "Epoch: 34, Cost: 0.8030014731945762\n",
      "Epoch: 35, Cost: 0.8019455386647949\n",
      "Epoch: 36, Cost: 0.8009909783188088\n",
      "Epoch: 37, Cost: 0.7999712996855812\n",
      "Epoch: 38, Cost: 0.7989015515945228\n",
      "Epoch: 39, Cost: 0.7978624327791658\n",
      "Epoch: 40, Cost: 0.7968214698595978\n",
      "Epoch: 41, Cost: 0.7957903985628046\n",
      "Epoch: 42, Cost: 0.7948092790226323\n",
      "Epoch: 43, Cost: 0.7938553633257065\n",
      "Epoch: 44, Cost: 0.7928305640812358\n",
      "Epoch: 45, Cost: 0.7918617133143468\n",
      "Epoch: 46, Cost: 0.7909263930350408\n",
      "Epoch: 47, Cost: 0.7900013650509233\n",
      "Epoch: 48, Cost: 0.7890716777503534\n",
      "Epoch: 49, Cost: 0.7881587861667504\n",
      "Epoch: 50, Cost: 0.7871564679819307\n",
      "Epoch: 51, Cost: 0.7861704238178068\n",
      "Epoch: 52, Cost: 0.7852662550931971\n",
      "Epoch: 53, Cost: 0.7843646388541177\n",
      "Epoch: 54, Cost: 0.7834853153443679\n",
      "Epoch: 55, Cost: 0.7825994898197075\n",
      "Epoch: 56, Cost: 0.781706393029126\n",
      "Epoch: 57, Cost: 0.7807844914585451\n",
      "Epoch: 58, Cost: 0.7799625396374077\n",
      "Epoch: 59, Cost: 0.7790658800786714\n",
      "Epoch: 60, Cost: 0.778217697737438\n",
      "Epoch: 61, Cost: 0.7773525371899486\n",
      "Epoch: 62, Cost: 0.7765082412476267\n",
      "Epoch: 63, Cost: 0.7756337882857215\n",
      "Epoch: 64, Cost: 0.7747965756033945\n",
      "Epoch: 65, Cost: 0.7739569803594991\n",
      "Epoch: 66, Cost: 0.7731682469039156\n",
      "Epoch: 67, Cost: 0.7723814834029562\n",
      "Epoch: 68, Cost: 0.7715661736745223\n",
      "Epoch: 69, Cost: 0.770772878846546\n",
      "Epoch: 70, Cost: 0.7700137082407557\n",
      "Epoch: 71, Cost: 0.769224652402422\n",
      "Epoch: 72, Cost: 0.7684155909495958\n",
      "Epoch: 73, Cost: 0.7676633857673758\n",
      "Epoch: 74, Cost: 0.7669037268689255\n",
      "Epoch: 75, Cost: 0.7661308432724906\n",
      "Epoch: 76, Cost: 0.7653826007955945\n",
      "Epoch: 77, Cost: 0.7646478492007742\n",
      "Epoch: 78, Cost: 0.7638970722738082\n",
      "Epoch: 79, Cost: 0.7631761612411164\n",
      "Epoch: 80, Cost: 0.7624674244818375\n",
      "Epoch: 81, Cost: 0.7617738965803424\n",
      "Epoch: 82, Cost: 0.7610565648723151\n",
      "Epoch: 83, Cost: 0.7603554588257018\n",
      "Epoch: 84, Cost: 0.7596167259919067\n",
      "Epoch: 85, Cost: 0.7589342070373649\n",
      "Epoch: 86, Cost: 0.7582798296467639\n",
      "Epoch: 87, Cost: 0.7575896456198443\n",
      "Epoch: 88, Cost: 0.7568316630541464\n",
      "Epoch: 89, Cost: 0.7561422218980283\n",
      "Epoch: 90, Cost: 0.7554853042450858\n",
      "Epoch: 91, Cost: 0.7548314904322238\n",
      "Epoch: 92, Cost: 0.754149639654227\n",
      "Epoch: 93, Cost: 0.7534813842038591\n",
      "Epoch: 94, Cost: 0.752816884190847\n",
      "Epoch: 95, Cost: 0.7522024404287858\n",
      "Epoch: 96, Cost: 0.7515547697967799\n",
      "Epoch: 97, Cost: 0.750935146927535\n",
      "Epoch: 98, Cost: 0.750297993138217\n",
      "Epoch: 99, Cost: 0.7496812035018049\n",
      "Epoch: 100, Cost: 0.7490702313255063\n",
      "Epoch: 101, Cost: 0.748385488013827\n",
      "Epoch: 102, Cost: 0.7478013250476908\n",
      "Epoch: 103, Cost: 0.7471896889849139\n",
      "Epoch: 104, Cost: 0.7465988260787276\n",
      "Epoch: 105, Cost: 0.7460019665103685\n",
      "Epoch: 106, Cost: 0.7453805550125093\n",
      "Epoch: 107, Cost: 0.744781440759774\n",
      "Epoch: 108, Cost: 0.7441858637882952\n",
      "Epoch: 109, Cost: 0.74359216291515\n",
      "Epoch: 110, Cost: 0.7430321558593592\n",
      "Epoch: 111, Cost: 0.7424393749986994\n",
      "Epoch: 112, Cost: 0.7419007516858295\n",
      "Epoch: 113, Cost: 0.7413244806014632\n",
      "Epoch: 114, Cost: 0.7407668180606457\n",
      "Epoch: 115, Cost: 0.7402479270972631\n",
      "Epoch: 116, Cost: 0.7396790703457602\n",
      "Epoch: 117, Cost: 0.739153996355876\n",
      "Epoch: 118, Cost: 0.7386786433521839\n",
      "Epoch: 119, Cost: 0.7380545338771708\n",
      "Epoch: 120, Cost: 0.7375506474003534\n",
      "Epoch: 121, Cost: 0.7370258767356731\n",
      "Epoch: 122, Cost: 0.7364638909815006\n",
      "Epoch: 123, Cost: 0.7359610621096618\n",
      "Epoch: 124, Cost: 0.7354767184064321\n",
      "Epoch: 125, Cost: 0.7349906268182885\n",
      "Epoch: 126, Cost: 0.7344813344826818\n",
      "Epoch: 127, Cost: 0.7339883118789069\n",
      "Epoch: 128, Cost: 0.7335128036648728\n",
      "Epoch: 129, Cost: 0.7330389148181244\n",
      "Epoch: 130, Cost: 0.7325647912382304\n",
      "Epoch: 131, Cost: 0.7321215096430579\n",
      "Epoch: 132, Cost: 0.7316123105392515\n",
      "Epoch: 133, Cost: 0.7311419144354481\n",
      "Epoch: 134, Cost: 0.7306806615533297\n",
      "Epoch: 135, Cost: 0.7302408588515102\n",
      "Epoch: 136, Cost: 0.7297748226480042\n",
      "Epoch: 137, Cost: 0.7293436281114616\n",
      "Epoch: 138, Cost: 0.7288491366512628\n",
      "Epoch: 139, Cost: 0.7284189323517207\n",
      "Epoch: 140, Cost: 0.7279745800570059\n",
      "Epoch: 141, Cost: 0.7275691860989488\n",
      "Epoch: 142, Cost: 0.7272048355567398\n",
      "Epoch: 143, Cost: 0.7266659988044123\n",
      "Epoch: 144, Cost: 0.7262332392919528\n",
      "Epoch: 145, Cost: 0.7258218459936602\n",
      "Epoch: 146, Cost: 0.7254088153697164\n",
      "Epoch: 147, Cost: 0.7249739226414086\n",
      "Epoch: 148, Cost: 0.7245649127610675\n",
      "Epoch: 149, Cost: 0.7241727831203723\n",
      "Epoch: 150, Cost: 0.7237569648944114\n",
      "Epoch: 151, Cost: 0.7234162150065374\n",
      "Epoch: 152, Cost: 0.723011615393159\n",
      "Epoch: 153, Cost: 0.7226302888299497\n",
      "Epoch: 154, Cost: 0.7222369381395503\n",
      "Epoch: 155, Cost: 0.7218605819935943\n",
      "Epoch: 156, Cost: 0.7214859133687888\n",
      "Epoch: 157, Cost: 0.7211120364940731\n",
      "Epoch: 158, Cost: 0.7207555495311727\n",
      "Epoch: 159, Cost: 0.7203691994935832\n",
      "Epoch: 160, Cost: 0.7200113435167714\n",
      "Epoch: 161, Cost: 0.7196458092089663\n",
      "Epoch: 162, Cost: 0.7192811169572811\n",
      "Epoch: 163, Cost: 0.718914074644192\n",
      "Epoch: 164, Cost: 0.7185564940468584\n",
      "Epoch: 165, Cost: 0.718260468338022\n",
      "Epoch: 166, Cost: 0.7178888270290659\n",
      "Epoch: 167, Cost: 0.7175164392456965\n",
      "Epoch: 168, Cost: 0.7171798282480063\n",
      "Epoch: 169, Cost: 0.7168337302344393\n",
      "Epoch: 170, Cost: 0.7164794651288248\n",
      "Epoch: 171, Cost: 0.7161531342474309\n",
      "Epoch: 172, Cost: 0.7158226424061733\n",
      "Epoch: 173, Cost: 0.7155056592376664\n",
      "Epoch: 174, Cost: 0.7152179481784924\n",
      "Epoch: 175, Cost: 0.7149254972228516\n",
      "Epoch: 176, Cost: 0.714601078587564\n",
      "Epoch: 177, Cost: 0.7142957738092255\n",
      "Epoch: 178, Cost: 0.7140019808101101\n",
      "Epoch: 179, Cost: 0.7136820210578522\n",
      "Epoch: 180, Cost: 0.7133812926091931\n",
      "Epoch: 181, Cost: 0.7130879124848591\n",
      "Epoch: 182, Cost: 0.7128186896470059\n",
      "Epoch: 183, Cost: 0.712506131514588\n",
      "Epoch: 184, Cost: 0.7122569755336802\n",
      "Epoch: 185, Cost: 0.7119773527480473\n",
      "Epoch: 186, Cost: 0.7116615583800735\n",
      "Epoch: 187, Cost: 0.7113828020200582\n",
      "Epoch: 188, Cost: 0.7111493725323025\n",
      "Epoch: 189, Cost: 0.7108692213325504\n",
      "Epoch: 190, Cost: 0.7105903327861519\n",
      "Epoch: 191, Cost: 0.7103607309715205\n",
      "Epoch: 192, Cost: 0.7100731199580373\n",
      "Epoch: 193, Cost: 0.7097941540576838\n",
      "Epoch: 194, Cost: 0.7095639946030747\n",
      "Epoch: 195, Cost: 0.7092629205098367\n",
      "Epoch: 196, Cost: 0.709028012346959\n",
      "Epoch: 197, Cost: 0.7087568328929549\n",
      "Epoch: 198, Cost: 0.7085795067649121\n",
      "Epoch: 199, Cost: 0.7083157950534267\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.49868142,  0.15022188, -0.44362927,  0.18889514, -0.26953698,\n",
       "        0.44280433,  0.11662468, -0.4676186 ,  0.16486704,  0.26265873,\n",
       "        0.66718414, -0.30938882, -0.59388329, -0.05175603, -0.6341884 ,\n",
       "       -0.25137628, -0.65076391,  0.58552154, -0.31337846, -0.43250244,\n",
       "       -1.05709188])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 定义逻辑斯蒂克（logistics）目标函数\n",
    "def logistics_function(X, y, weights):\n",
    "    z = np.dot(X, weights)\n",
    "    predictions = 1 / (1 + np.exp(-z))\n",
    "    loss = -np.mean(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))\n",
    "    return loss\n",
    "\n",
    "# 计算逻辑斯蒂克（logistics）目标函数关于参数的梯度\n",
    "def gradient(X, y, weights):\n",
    "    z = np.dot(X, weights)\n",
    "    predictions = 1 / (1 + np.exp(-z))\n",
    "    error = predictions - y\n",
    "    grad = np.dot(X.T, error) / len(X)\n",
    "    return grad\n",
    "\n",
    "# 小批量梯度下降算法\n",
    "def mini_batch_gradient_descent(X, y, learning_rate=0.01, batch_size=32, num_epochs=100):\n",
    "    # 初始化参数\n",
    "    weights = np.random.randn(X.shape[1]) ##\n",
    "    \n",
    "    # 迭代训练\n",
    "    for epoch in range(num_epochs):\n",
    "        # 在每个epoch开始时，对训练数据进行洗牌\n",
    "        shuffled_indices = np.random.permutation(len(X))\n",
    "        X_shuffled = X[shuffled_indices]\n",
    "        y_shuffled = y[shuffled_indices]\n",
    "        \n",
    "        # 按批次更新参数\n",
    "        for batch_start in range(0, len(X), batch_size):\n",
    "            # 获取当前批次的数据\n",
    "            X_batch = X_shuffled[batch_start:batch_start+batch_size]\n",
    "            y_batch = y_shuffled[batch_start:batch_start+batch_size]\n",
    "            losses = []  # 记录损失函数值  --0731add\n",
    "            gradients = []  # 记录梯度范数 --0731add\n",
    "            # 计算梯度\n",
    "            grad = gradient(X_batch, y_batch, weights)       \n",
    "            # 更新参数\n",
    "            weights -= learning_rate * grad\n",
    "            loss_iteration=logistics_function(X, y, weights) #--0731add\n",
    "            losses.append(loss_iteration)   #--0731add\n",
    "            gradients.append(np.linalg.norm(grad))  # 计算梯度范数并记录\n",
    "            \n",
    "        # 计算当前参数下的目标函数值\n",
    "        cost = logistics_function(X, y, weights)\n",
    "        \n",
    "        # 输出损失\n",
    "        print(\"Epoch: {}, Cost: {}\".format(epoch, cost))\n",
    "    return weights\n",
    "    \n",
    "\n",
    "# # 创建训练数据（示例数据，自行替换）\n",
    "# np.random.seed(0)\n",
    "# X = np.random.randn(1000, 3)\n",
    "# y = np.random.randint(2, size=1000)\n",
    "\n",
    "# # 添加偏置项\n",
    "# X_bias = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "# 运行小批量梯度下降算法\n",
    "mini_batch_gradient_descent(X_bias, y, learning_rate=0.01, batch_size=32, num_epochs=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cost: 1.3096533499586585\n",
      "Epoch: 1, Cost: 1.12059994255397\n",
      "Epoch: 2, Cost: 1.0047190193198514\n",
      "Epoch: 3, Cost: 0.9343446492898251\n",
      "Epoch: 4, Cost: 0.8944978992568036\n",
      "Epoch: 5, Cost: 0.8752342277194701\n",
      "Epoch: 6, Cost: 0.8637996671793882\n",
      "Epoch: 7, Cost: 0.8570889102138148\n",
      "Epoch: 8, Cost: 0.8528503196339836\n",
      "Epoch: 9, Cost: 0.8501181316862969\n",
      "Epoch: 10, Cost: 0.8480212194753907\n",
      "Epoch: 11, Cost: 0.845909273243948\n",
      "Epoch: 12, Cost: 0.844045870902304\n",
      "Epoch: 13, Cost: 0.8422098397315247\n",
      "Epoch: 14, Cost: 0.8406860239848936\n",
      "Epoch: 15, Cost: 0.839052014501159\n",
      "Epoch: 16, Cost: 0.8374819403658383\n",
      "Epoch: 17, Cost: 0.835934139332305\n",
      "Epoch: 18, Cost: 0.834434455478051\n",
      "Epoch: 19, Cost: 0.8329294455719033\n",
      "Epoch: 20, Cost: 0.8314228486831742\n",
      "Epoch: 21, Cost: 0.829838840939376\n",
      "Epoch: 22, Cost: 0.8283796060526519\n",
      "Epoch: 23, Cost: 0.82688306416501\n",
      "Epoch: 24, Cost: 0.8254565468803555\n",
      "Epoch: 25, Cost: 0.8240115736769243\n",
      "Epoch: 26, Cost: 0.8226274866059969\n",
      "Epoch: 27, Cost: 0.8211197815506789\n",
      "Epoch: 28, Cost: 0.8196772427473537\n",
      "Epoch: 29, Cost: 0.8182842203706607\n",
      "Epoch: 30, Cost: 0.8168955668840747\n",
      "Epoch: 31, Cost: 0.8154772623946988\n",
      "Epoch: 32, Cost: 0.8141163528440158\n",
      "Epoch: 33, Cost: 0.8127486417579424\n",
      "Epoch: 34, Cost: 0.8113418612094174\n",
      "Epoch: 35, Cost: 0.8099345996625689\n",
      "Epoch: 36, Cost: 0.8085819189135303\n",
      "Epoch: 37, Cost: 0.8072736561778504\n",
      "Epoch: 38, Cost: 0.8059506469068656\n",
      "Epoch: 39, Cost: 0.8045379965831414\n",
      "Epoch: 40, Cost: 0.8032257279145933\n",
      "Epoch: 41, Cost: 0.8019729721393474\n",
      "Epoch: 42, Cost: 0.8007285700950927\n",
      "Epoch: 43, Cost: 0.7994184635296621\n",
      "Epoch: 44, Cost: 0.7981841816347751\n",
      "Epoch: 45, Cost: 0.7969529246261955\n",
      "Epoch: 46, Cost: 0.7957280133738818\n",
      "Epoch: 47, Cost: 0.794535848899787\n",
      "Epoch: 48, Cost: 0.7933150581903922\n",
      "Epoch: 49, Cost: 0.7921276872499052\n",
      "Epoch: 50, Cost: 0.7909515657468026\n",
      "Epoch: 51, Cost: 0.7897913130187056\n",
      "Epoch: 52, Cost: 0.7886498112938707\n",
      "Epoch: 53, Cost: 0.7874459275181023\n",
      "Epoch: 54, Cost: 0.7863356006803773\n",
      "Epoch: 55, Cost: 0.7852194166807357\n",
      "Epoch: 56, Cost: 0.7841458916947455\n",
      "Epoch: 57, Cost: 0.7829922361155748\n",
      "Epoch: 58, Cost: 0.7818843559958764\n",
      "Epoch: 59, Cost: 0.7808130106009356\n",
      "Epoch: 60, Cost: 0.7798459042772\n",
      "Epoch: 61, Cost: 0.7788389298707991\n",
      "Epoch: 62, Cost: 0.7777025385066725\n",
      "Epoch: 63, Cost: 0.7766800695315758\n",
      "Epoch: 64, Cost: 0.7755414312171894\n",
      "Epoch: 65, Cost: 0.7744469713423442\n",
      "Epoch: 66, Cost: 0.7734112706024566\n",
      "Epoch: 67, Cost: 0.7723781107260793\n",
      "Epoch: 68, Cost: 0.7713604887639104\n",
      "Epoch: 69, Cost: 0.7703857246020657\n",
      "Epoch: 70, Cost: 0.7694258142388999\n",
      "Epoch: 71, Cost: 0.7684577918452791\n",
      "Epoch: 72, Cost: 0.7674464671512053\n",
      "Epoch: 73, Cost: 0.7665405556935564\n",
      "Epoch: 74, Cost: 0.7655286577213185\n",
      "Epoch: 75, Cost: 0.7645975023199522\n",
      "Epoch: 76, Cost: 0.7637155814780767\n",
      "Epoch: 77, Cost: 0.7628608456407223\n",
      "Epoch: 78, Cost: 0.7619621863719913\n",
      "Epoch: 79, Cost: 0.761076004783461\n",
      "Epoch: 80, Cost: 0.7601786011530224\n",
      "Epoch: 81, Cost: 0.759329993283438\n",
      "Epoch: 82, Cost: 0.7584562969832179\n",
      "Epoch: 83, Cost: 0.7576116431479633\n",
      "Epoch: 84, Cost: 0.7567553129348223\n",
      "Epoch: 85, Cost: 0.7559577930089417\n",
      "Epoch: 86, Cost: 0.7550874395489423\n",
      "Epoch: 87, Cost: 0.7542846697958381\n",
      "Epoch: 88, Cost: 0.7534545097489468\n",
      "Epoch: 89, Cost: 0.7525965292600073\n",
      "Epoch: 90, Cost: 0.7517690722361943\n",
      "Epoch: 91, Cost: 0.7509918427610168\n",
      "Epoch: 92, Cost: 0.7502273503256126\n",
      "Epoch: 93, Cost: 0.7494678568287155\n",
      "Epoch: 94, Cost: 0.7486797439348418\n",
      "Epoch: 95, Cost: 0.7479297725837591\n",
      "Epoch: 96, Cost: 0.7471839550797872\n",
      "Epoch: 97, Cost: 0.746456704058085\n",
      "Epoch: 98, Cost: 0.7456937699996673\n",
      "Epoch: 99, Cost: 0.7449629580144768\n",
      "Epoch: 100, Cost: 0.7442499229383409\n",
      "Epoch: 101, Cost: 0.743537076202198\n",
      "Epoch: 102, Cost: 0.7428480019785308\n",
      "Epoch: 103, Cost: 0.7421092250173621\n",
      "Epoch: 104, Cost: 0.7414341393171168\n",
      "Epoch: 105, Cost: 0.7407685625932126\n",
      "Epoch: 106, Cost: 0.7400645510376556\n",
      "Epoch: 107, Cost: 0.7394426126548326\n",
      "Epoch: 108, Cost: 0.7388013121691058\n",
      "Epoch: 109, Cost: 0.7381231753188695\n",
      "Epoch: 110, Cost: 0.7374688232131079\n",
      "Epoch: 111, Cost: 0.7368289026731969\n",
      "Epoch: 112, Cost: 0.736201707386253\n",
      "Epoch: 113, Cost: 0.7355516019977699\n",
      "Epoch: 114, Cost: 0.7349337115970689\n",
      "Epoch: 115, Cost: 0.7343328411306804\n",
      "Epoch: 116, Cost: 0.7337992807736221\n",
      "Epoch: 117, Cost: 0.7331089140012841\n",
      "Epoch: 118, Cost: 0.7325585278396384\n",
      "Epoch: 119, Cost: 0.7319587547183419\n",
      "Epoch: 120, Cost: 0.731356732869075\n",
      "Epoch: 121, Cost: 0.7307704938474613\n",
      "Epoch: 122, Cost: 0.7302343560536795\n",
      "Epoch: 123, Cost: 0.7296687333853894\n",
      "Epoch: 124, Cost: 0.7290836775487748\n",
      "Epoch: 125, Cost: 0.7285286737107431\n",
      "Epoch: 126, Cost: 0.7279763503584631\n",
      "Epoch: 127, Cost: 0.7274640931270103\n",
      "Epoch: 128, Cost: 0.7269409632511353\n",
      "Epoch: 129, Cost: 0.7264247609608083\n",
      "Epoch: 130, Cost: 0.7259243094488672\n",
      "Epoch: 131, Cost: 0.7254095608369159\n",
      "Epoch: 132, Cost: 0.7249257961830831\n",
      "Epoch: 133, Cost: 0.7244146421192698\n",
      "Epoch: 134, Cost: 0.7239235510119456\n",
      "Epoch: 135, Cost: 0.7234190533709809\n",
      "Epoch: 136, Cost: 0.722963980850823\n",
      "Epoch: 137, Cost: 0.7224884182482804\n",
      "Epoch: 138, Cost: 0.7220431919974096\n",
      "Epoch: 139, Cost: 0.7216083642869139\n",
      "Epoch: 140, Cost: 0.7211117022797043\n",
      "Epoch: 141, Cost: 0.7206580730986776\n",
      "Epoch: 142, Cost: 0.7202136837268217\n",
      "Epoch: 143, Cost: 0.7197743760744647\n",
      "Epoch: 144, Cost: 0.7193700580817081\n",
      "Epoch: 145, Cost: 0.7189385175899786\n",
      "Epoch: 146, Cost: 0.7185188383676462\n",
      "Epoch: 147, Cost: 0.7180734295512243\n",
      "Epoch: 148, Cost: 0.717636306279311\n",
      "Epoch: 149, Cost: 0.7172335434411714\n",
      "Epoch: 150, Cost: 0.7168576757990982\n",
      "Epoch: 151, Cost: 0.7163892114505108\n",
      "Epoch: 152, Cost: 0.7159882904219554\n",
      "Epoch: 153, Cost: 0.7156162045236736\n",
      "Epoch: 154, Cost: 0.7152459543775305\n",
      "Epoch: 155, Cost: 0.7148425583367439\n",
      "Epoch: 156, Cost: 0.7144908598769498\n",
      "Epoch: 157, Cost: 0.7141297063234019\n",
      "Epoch: 158, Cost: 0.7137567966009633\n",
      "Epoch: 159, Cost: 0.713392678004104\n",
      "Epoch: 160, Cost: 0.7130421611384168\n",
      "Epoch: 161, Cost: 0.7126864911012813\n",
      "Epoch: 162, Cost: 0.7123259387857868\n",
      "Epoch: 163, Cost: 0.7119848241787354\n",
      "Epoch: 164, Cost: 0.7116381583468216\n",
      "Epoch: 165, Cost: 0.7112736759317229\n",
      "Epoch: 166, Cost: 0.7109594664756657\n",
      "Epoch: 167, Cost: 0.7106382425895088\n",
      "Epoch: 168, Cost: 0.7103181962747119\n",
      "Epoch: 169, Cost: 0.7099937693244724\n",
      "Epoch: 170, Cost: 0.7096642702324314\n",
      "Epoch: 171, Cost: 0.709372152343557\n",
      "Epoch: 172, Cost: 0.7090759154088376\n",
      "Epoch: 173, Cost: 0.7087441518070595\n",
      "Epoch: 174, Cost: 0.7084286249388344\n",
      "Epoch: 175, Cost: 0.7081193653838044\n",
      "Epoch: 176, Cost: 0.707825838711932\n",
      "Epoch: 177, Cost: 0.7075612674950188\n",
      "Epoch: 178, Cost: 0.7072679882281112\n",
      "Epoch: 179, Cost: 0.7069941046708221\n",
      "Epoch: 180, Cost: 0.706699461510325\n",
      "Epoch: 181, Cost: 0.7064351046744509\n",
      "Epoch: 182, Cost: 0.7061548276826748\n",
      "Epoch: 183, Cost: 0.7059009198443827\n",
      "Epoch: 184, Cost: 0.7056083129831929\n",
      "Epoch: 185, Cost: 0.7053373895591614\n",
      "Epoch: 186, Cost: 0.7050857344296192\n",
      "Epoch: 187, Cost: 0.7048309571826243\n",
      "Epoch: 188, Cost: 0.7046101082139187\n",
      "Epoch: 189, Cost: 0.704311357129252\n",
      "Epoch: 190, Cost: 0.7040467951714597\n",
      "Epoch: 191, Cost: 0.7037874539312237\n",
      "Epoch: 192, Cost: 0.7035659780925838\n",
      "Epoch: 193, Cost: 0.70332233347981\n",
      "Epoch: 194, Cost: 0.7030818811054265\n",
      "Epoch: 195, Cost: 0.7028532686691209\n",
      "Epoch: 196, Cost: 0.7026284404010359\n",
      "Epoch: 197, Cost: 0.7024194828968642\n",
      "Epoch: 198, Cost: 0.7022187155478259\n",
      "Epoch: 199, Cost: 0.7019293655987949\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.05948316e+00,  2.94291600e-01, -8.79443941e-02,  7.98811782e-01,\n",
       "        4.96506000e-02,  4.08725033e-01, -5.15544910e-01,  8.65423452e-02,\n",
       "       -3.92528965e-04,  4.84422262e-01,  1.37085501e-01, -5.61794998e-02,\n",
       "        4.26863724e-03, -2.88995799e-01, -3.85001106e-01,  1.84249664e-01,\n",
       "       -2.40840442e-02,  9.15144955e-01,  4.48760020e-01, -2.71688550e-02,\n",
       "       -1.30481701e-01])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 定义目标函数\n",
    "def loss_function(X, y, weights):\n",
    "    z = np.dot(X, weights)\n",
    "    predictions = 1 / (1 + np.exp(-z))\n",
    "    loss = -np.mean(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))\n",
    "    return loss\n",
    "\n",
    "# 计算目标函数关于参数的梯度\n",
    "def gradient(X, y, weights):\n",
    "    z = np.dot(X, weights)\n",
    "    predictions = 1 / (1 + np.exp(-z))\n",
    "    error = predictions - y\n",
    "    grad = np.dot(X.T, error) / len(X)\n",
    "    return grad\n",
    "\n",
    "# KL散度的计算函数\n",
    "# def kl_divergence(params1, params2):\n",
    "#     # 根据模型参数计算概率分布\n",
    "#     # ...\n",
    "\n",
    "#     # 计算KL散度\n",
    "#     # kl = ...\n",
    "#     kl = 0  # 这里需要根据具体的模型参数和KL散度的定义进行计算\n",
    "#     return kl\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x)\n",
    "    # 使用Softmax函数将旧模型参数映射到概率分布\n",
    "\n",
    "\n",
    "def kl_divergence(params1,params2):\n",
    " \n",
    "   # 计算离散概率分布的KL散度\n",
    "    oldparams_prob = softmax(params1)\n",
    "    batchparams_prob = softmax(params2)\n",
    "    kl = np.sum(np.array(oldparams_prob) * np.log(np.array(oldparams_prob)/np.array(batchparams_prob)))\n",
    "    return kl\n",
    "\n",
    "# 小批量梯度下降算法\n",
    "def mini_batch_gradient_descent(X, y, learning_rate=0.01, batch_size=32, num_epochs=100, regularization_weight=0.01):\n",
    "    # 初始化参数\n",
    "    weights = np.random.randn(X.shape[1])\n",
    "\n",
    "    # 保存上一次的参数\n",
    "    prev_weights = weights.copy()\n",
    "    \n",
    "    # 迭代训练\n",
    "    for epoch in range(num_epochs):\n",
    "        # 在每个epoch开始时，对训练数据进行洗牌\n",
    "        shuffled_indices = np.random.permutation(len(X))\n",
    "        X_shuffled = X[shuffled_indices]\n",
    "        y_shuffled = y[shuffled_indices]\n",
    "        \n",
    "        # 按批次更新参数\n",
    "        for batch_start in range(0, len(X), batch_size):\n",
    "            # 获取当前批次的数据\n",
    "            X_batch = X_shuffled[batch_start:batch_start+batch_size]\n",
    "            y_batch = y_shuffled[batch_start:batch_start+batch_size]\n",
    "            \n",
    "            # 计算梯度\n",
    "            grad = gradient(X_batch, y_batch, weights)\n",
    "            \n",
    "            # 计算KL散度\n",
    "            kl = kl_divergence(weights, prev_weights)\n",
    "            \n",
    "            # 更新参数\n",
    "            weights -= learning_rate * grad + regularization_weight * kl\n",
    "            \n",
    "            # 更新上一次的参数\n",
    "            prev_weights = weights.copy()\n",
    "            \n",
    "        # 计算当前参数下的目标函数值\n",
    "        cost = loss_function(X, y, weights)\n",
    "        \n",
    "        # 输出损失\n",
    "        print(\"Epoch: {}, Cost: {}\".format(epoch, cost))\n",
    "    return weights\n",
    "# 创建训练数据（示例数据，自行替换）\n",
    "np.random.seed(0)\n",
    "# X = np.random.randn(1000, 3)\n",
    "X= np.random.random(size=(1000,20))\n",
    "y = np.random.randint(2, size=1000)\n",
    "\n",
    "# 添加偏置项\n",
    "X_bias = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "# 运行小批量梯度下降算法\n",
    "mini_batch_gradient_descent(X_bias, y, learning_rate=0.01, batch_size=32, num_epochs=200, regularization_weight=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.352148476070165"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oldparams_prob=np.random.randn(10)\n",
    "# batchparams_prob=np.random.randn(10)\n",
    "oldparams_prob=np.random.random(10)\n",
    "batchparams_prob=np.random.random(10)\n",
    "kl = np.sum(np.array(oldparams_prob) * np.log(np.array(oldparams_prob)/np.array(batchparams_prob)))\n",
    "kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.21815318, 0.59840151, 0.41057459, 0.46041553, 0.66998668,\n",
       "       0.3596342 , 0.24065582, 0.91822376, 0.34518809, 0.47802094])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL散度值为: 0.05232481437645474\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    \"\"\"\n",
    "    计算离散概率分布的KL散度\n",
    "\n",
    "    参数：\n",
    "        p: numpy.array，表示概率分布P，要求所有元素都大于0，且和为1\n",
    "        q: numpy.array，表示概率分布Q，要求所有元素都大于0，且和为1\n",
    "\n",
    "    返回：\n",
    "        kl: float，KL散度值\n",
    "    \"\"\"\n",
    "    assert len(p) == len(q), \"概率分布的长度不一致\"\n",
    "    assert np.all(p > 0) and np.all(q > 0), \"概率分布的所有元素必须大于0\"\n",
    "    assert np.isclose(np.sum(p), 1.0) and np.isclose(np.sum(q), 1.0), \"概率分布的和必须为1\"\n",
    "\n",
    "    kl = np.sum(p * np.log(p / q))\n",
    "    return kl\n",
    "\n",
    "# 示例：计算两个概率分布的KL散度\n",
    "p = np.array([0.2, 0.3, 0.5])\n",
    "q = np.array([0.1, 0.4, 0.5])\n",
    "kl_value = kl_divergence(p, q)\n",
    "print(\"KL散度值为:\", kl_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "旧模型参数： [1. 2. 3. 4.]\n",
      "旧模型参数概率分布： [0.0320586  0.08714432 0.23688282 0.64391426]\n",
      "概率分布的和： 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "# 旧模型参数向量\n",
    "old_params = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "\n",
    "# 使用Softmax函数将旧模型参数映射到概率分布\n",
    "old_params_prob = softmax(old_params)\n",
    "\n",
    "print(\"旧模型参数：\", old_params)\n",
    "print(\"旧模型参数概率分布：\", old_params_prob)\n",
    "print(\"概率分布的和：\", np.sum(old_params_prob))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cost: 1.1149677056433556\n",
      "Epoch: 1, Cost: 1.087996620117277\n",
      "Epoch: 2, Cost: 1.0618960821422725\n",
      "Epoch: 3, Cost: 1.0358900921524437\n",
      "Epoch: 4, Cost: 1.0114994150846943\n",
      "Epoch: 5, Cost: 0.9880123484141561\n",
      "Epoch: 6, Cost: 0.9656273186595019\n",
      "Epoch: 7, Cost: 0.9438761708862489\n",
      "Epoch: 8, Cost: 0.9234876831400546\n",
      "Epoch: 9, Cost: 0.9042946658740616\n",
      "Epoch: 10, Cost: 0.8858925180478097\n",
      "Epoch: 11, Cost: 0.8683655856812188\n",
      "Epoch: 12, Cost: 0.8519484795570893\n",
      "Epoch: 13, Cost: 0.8371306215860157\n",
      "Epoch: 14, Cost: 0.8227714090892063\n",
      "Epoch: 15, Cost: 0.8090556278775695\n",
      "Epoch: 16, Cost: 0.7970894191154226\n",
      "Epoch: 17, Cost: 0.7860604913596378\n",
      "Epoch: 18, Cost: 0.7760371688721439\n",
      "Epoch: 19, Cost: 0.7668103637428171\n",
      "Epoch: 20, Cost: 0.7585038392224709\n",
      "Epoch: 21, Cost: 0.7509655207238668\n",
      "Epoch: 22, Cost: 0.7441039105333611\n",
      "Epoch: 23, Cost: 0.7375396328954794\n",
      "Epoch: 24, Cost: 0.7320652887315464\n",
      "Epoch: 25, Cost: 0.7268913012033891\n",
      "Epoch: 26, Cost: 0.7225576881214972\n",
      "Epoch: 27, Cost: 0.7185723006641379\n",
      "Epoch: 28, Cost: 0.7146039269920984\n",
      "Epoch: 29, Cost: 0.7114552883422492\n",
      "Epoch: 30, Cost: 0.7086649412369634\n",
      "Epoch: 31, Cost: 0.7062297131158141\n",
      "Epoch: 32, Cost: 0.7041153349507695\n",
      "Epoch: 33, Cost: 0.7022881000129089\n",
      "Epoch: 34, Cost: 0.7005456462368079\n",
      "Epoch: 35, Cost: 0.6990605261449362\n",
      "Epoch: 36, Cost: 0.6977493884669669\n",
      "Epoch: 37, Cost: 0.696588933391329\n",
      "Epoch: 38, Cost: 0.6954979062811518\n",
      "Epoch: 39, Cost: 0.6947403723632933\n",
      "Epoch: 40, Cost: 0.6939951424783767\n",
      "Epoch: 41, Cost: 0.6934603500712989\n",
      "Epoch: 42, Cost: 0.6929244953196734\n",
      "Epoch: 43, Cost: 0.69244083690653\n",
      "Epoch: 44, Cost: 0.6919798779080402\n",
      "Epoch: 45, Cost: 0.6916161907756271\n",
      "Epoch: 46, Cost: 0.6913595652249317\n",
      "Epoch: 47, Cost: 0.6910771243223905\n",
      "Epoch: 48, Cost: 0.6907874951531373\n",
      "Epoch: 49, Cost: 0.6905776085255325\n",
      "Epoch: 50, Cost: 0.690356159412237\n",
      "Epoch: 51, Cost: 0.6901938225047731\n",
      "Epoch: 52, Cost: 0.6899768168449849\n",
      "Epoch: 53, Cost: 0.6898523688780066\n",
      "Epoch: 54, Cost: 0.6897324836742326\n",
      "Epoch: 55, Cost: 0.6896545164243464\n",
      "Epoch: 56, Cost: 0.6895657060324065\n",
      "Epoch: 57, Cost: 0.6894989249908332\n",
      "Epoch: 58, Cost: 0.6894474398360145\n",
      "Epoch: 59, Cost: 0.6894374901879196\n",
      "Epoch: 60, Cost: 0.6894006322509264\n",
      "Epoch: 61, Cost: 0.6893409958738834\n",
      "Epoch: 62, Cost: 0.6893062631000401\n",
      "Epoch: 63, Cost: 0.689293049838697\n",
      "Epoch: 64, Cost: 0.6892766219813675\n",
      "Epoch: 65, Cost: 0.6892350836224789\n",
      "Epoch: 66, Cost: 0.6892327099204565\n",
      "Epoch: 67, Cost: 0.6892121913293613\n",
      "Epoch: 68, Cost: 0.6892000104368078\n",
      "Epoch: 69, Cost: 0.6891920660073568\n",
      "Epoch: 70, Cost: 0.6891812099314178\n",
      "Epoch: 71, Cost: 0.6891738446503155\n",
      "Epoch: 72, Cost: 0.6891662401988092\n",
      "Epoch: 73, Cost: 0.6891561015998698\n",
      "Epoch: 74, Cost: 0.6891519117165054\n",
      "Epoch: 75, Cost: 0.6891468335982284\n",
      "Epoch: 76, Cost: 0.6891458881860845\n",
      "Epoch: 77, Cost: 0.6891422825890209\n",
      "Epoch: 78, Cost: 0.6891433795939403\n",
      "Epoch: 79, Cost: 0.6891448236158332\n",
      "Epoch: 80, Cost: 0.6891454266048735\n",
      "Epoch: 81, Cost: 0.6891422377325223\n",
      "Epoch: 82, Cost: 0.6891418363261951\n",
      "Epoch: 83, Cost: 0.6891450866393836\n",
      "Epoch: 84, Cost: 0.6891494978253047\n",
      "Epoch: 85, Cost: 0.6891518692402537\n",
      "Epoch: 86, Cost: 0.6891500689848007\n",
      "Epoch: 87, Cost: 0.689148377251649\n",
      "Epoch: 88, Cost: 0.6891435551926757\n",
      "Epoch: 89, Cost: 0.6891458484394056\n",
      "Epoch: 90, Cost: 0.6891435945724763\n",
      "Epoch: 91, Cost: 0.6891483218095252\n",
      "Epoch: 92, Cost: 0.6891485862186616\n",
      "Epoch: 93, Cost: 0.6891520392235788\n",
      "Epoch: 94, Cost: 0.6891463694753848\n",
      "Epoch: 95, Cost: 0.689150355458026\n",
      "Epoch: 96, Cost: 0.6891520261000499\n",
      "Epoch: 97, Cost: 0.6891534708005644\n",
      "Epoch: 98, Cost: 0.6891518966865439\n",
      "Epoch: 99, Cost: 0.6891448258880823\n",
      "Epoch: 100, Cost: 0.6891400704207069\n",
      "Epoch: 101, Cost: 0.6891419900277326\n",
      "Epoch: 102, Cost: 0.6891424532703805\n",
      "Epoch: 103, Cost: 0.6891409519744084\n",
      "Epoch: 104, Cost: 0.6891459416617981\n",
      "Epoch: 105, Cost: 0.6891459295119137\n",
      "Epoch: 106, Cost: 0.6891474042979933\n",
      "Epoch: 107, Cost: 0.6891472225602232\n",
      "Epoch: 108, Cost: 0.6891457787463644\n",
      "Epoch: 109, Cost: 0.6891463449132776\n",
      "Epoch: 110, Cost: 0.6891398515358815\n",
      "Epoch: 111, Cost: 0.689136878870708\n",
      "Epoch: 112, Cost: 0.6891359583566126\n",
      "Epoch: 113, Cost: 0.6891339813752623\n",
      "Epoch: 114, Cost: 0.6891344109634385\n",
      "Epoch: 115, Cost: 0.6891346814347143\n",
      "Epoch: 116, Cost: 0.6891354674393069\n",
      "Epoch: 117, Cost: 0.6891342851227493\n",
      "Epoch: 118, Cost: 0.6891341624225763\n",
      "Epoch: 119, Cost: 0.6891332553809538\n",
      "Epoch: 120, Cost: 0.689134057422227\n",
      "Epoch: 121, Cost: 0.6891352856812255\n",
      "Epoch: 122, Cost: 0.6891354808734692\n",
      "Epoch: 123, Cost: 0.6891342027244718\n",
      "Epoch: 124, Cost: 0.6891351589745984\n",
      "Epoch: 125, Cost: 0.6891382199591853\n",
      "Epoch: 126, Cost: 0.6891371214961008\n",
      "Epoch: 127, Cost: 0.6891356378835697\n",
      "Epoch: 128, Cost: 0.6891357917919761\n",
      "Epoch: 129, Cost: 0.6891337709904296\n",
      "Epoch: 130, Cost: 0.6891367058361293\n",
      "Epoch: 131, Cost: 0.6891389623579339\n",
      "Epoch: 132, Cost: 0.6891390976677818\n",
      "Epoch: 133, Cost: 0.6891366059876508\n",
      "Epoch: 134, Cost: 0.6891369429799612\n",
      "Epoch: 135, Cost: 0.6891343094816332\n",
      "Epoch: 136, Cost: 0.6891333409714816\n",
      "Epoch: 137, Cost: 0.689132655352634\n",
      "Epoch: 138, Cost: 0.6891322943928533\n",
      "Epoch: 139, Cost: 0.6891334708955479\n",
      "Epoch: 140, Cost: 0.6891325385016664\n",
      "Epoch: 141, Cost: 0.6891325964354053\n",
      "Epoch: 142, Cost: 0.6891347328553324\n",
      "Epoch: 143, Cost: 0.6891338451866066\n",
      "Epoch: 144, Cost: 0.6891370380271384\n",
      "Epoch: 145, Cost: 0.6891347955914209\n",
      "Epoch: 146, Cost: 0.6891349085224194\n",
      "Epoch: 147, Cost: 0.6891341546442744\n",
      "Epoch: 148, Cost: 0.6891338146972736\n",
      "Epoch: 149, Cost: 0.6891351097479606\n",
      "Epoch: 150, Cost: 0.6891354026189681\n",
      "Epoch: 151, Cost: 0.6891363159447068\n",
      "Epoch: 152, Cost: 0.6891356778559362\n",
      "Epoch: 153, Cost: 0.6891376542910977\n",
      "Epoch: 154, Cost: 0.6891378109413658\n",
      "Epoch: 155, Cost: 0.6891371030644304\n",
      "Epoch: 156, Cost: 0.6891391393308139\n",
      "Epoch: 157, Cost: 0.6891373167334\n",
      "Epoch: 158, Cost: 0.6891391335720165\n",
      "Epoch: 159, Cost: 0.6891397688048835\n",
      "Epoch: 160, Cost: 0.6891398053131264\n",
      "Epoch: 161, Cost: 0.6891396187773466\n",
      "Epoch: 162, Cost: 0.6891386900794971\n",
      "Epoch: 163, Cost: 0.6891422396319212\n",
      "Epoch: 164, Cost: 0.68914239088823\n",
      "Epoch: 165, Cost: 0.6891434856423266\n",
      "Epoch: 166, Cost: 0.6891404766385508\n",
      "Epoch: 167, Cost: 0.6891403324700788\n",
      "Epoch: 168, Cost: 0.6891428323639528\n",
      "Epoch: 169, Cost: 0.6891394176078839\n",
      "Epoch: 170, Cost: 0.6891344859411589\n",
      "Epoch: 171, Cost: 0.6891332441863636\n",
      "Epoch: 172, Cost: 0.6891329496852059\n",
      "Epoch: 173, Cost: 0.6891332558202383\n",
      "Epoch: 174, Cost: 0.6891335842996936\n",
      "Epoch: 175, Cost: 0.6891328566860041\n",
      "Epoch: 176, Cost: 0.6891345145427175\n",
      "Epoch: 177, Cost: 0.6891340572470228\n",
      "Epoch: 178, Cost: 0.6891345260187488\n",
      "Epoch: 179, Cost: 0.6891348307181374\n",
      "Epoch: 180, Cost: 0.6891337279337512\n",
      "Epoch: 181, Cost: 0.689132671843392\n",
      "Epoch: 182, Cost: 0.6891361373941819\n",
      "Epoch: 183, Cost: 0.6891356549501176\n",
      "Epoch: 184, Cost: 0.6891341656311445\n",
      "Epoch: 185, Cost: 0.6891347607180128\n",
      "Epoch: 186, Cost: 0.6891337648972713\n",
      "Epoch: 187, Cost: 0.6891330532170861\n",
      "Epoch: 188, Cost: 0.6891323373925022\n",
      "Epoch: 189, Cost: 0.689132494863498\n",
      "Epoch: 190, Cost: 0.6891336642778659\n",
      "Epoch: 191, Cost: 0.6891324489727\n",
      "Epoch: 192, Cost: 0.6891328567540879\n",
      "Epoch: 193, Cost: 0.689132475686843\n",
      "Epoch: 194, Cost: 0.689136346739674\n",
      "Epoch: 195, Cost: 0.6891344807491171\n",
      "Epoch: 196, Cost: 0.6891380322964444\n",
      "Epoch: 197, Cost: 0.6891383554617722\n",
      "Epoch: 198, Cost: 0.6891346294863199\n",
      "Epoch: 199, Cost: 0.689134266092015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.14333691, -0.05989198,  0.07544325, -0.06099479])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 定义逻辑斯蒂克（logistics）目标函数\n",
    "def logistics_function(X, y, weights):\n",
    "    z = np.dot(X, weights)\n",
    "    predictions = 1 / (1 + np.exp(-z))\n",
    "    loss = -np.mean(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))\n",
    "    return loss\n",
    "\n",
    "# 计算逻辑斯蒂克（logistics）目标函数关于参数的梯度\n",
    "def gradient(X, y, weights):\n",
    "    z = np.dot(X, weights)\n",
    "    predictions = 1 / (1 + np.exp(-z))\n",
    "    error = predictions - y\n",
    "    grad = np.dot(X.T, error) / len(X)\n",
    "    return grad\n",
    "\n",
    "# 小批量梯度下降算法\n",
    "def mini_batch_gradient_descent(X, y, learning_rate=0.01, batch_size=32, num_epochs=100):\n",
    "    # 初始化参数\n",
    "    weights = np.random.randn(X.shape[1])\n",
    "    \n",
    "    # 迭代训练\n",
    "    for epoch in range(num_epochs):\n",
    "        # 在每个epoch开始时，对训练数据进行洗牌\n",
    "        shuffled_indices = np.random.permutation(len(X))\n",
    "        X_shuffled = X[shuffled_indices]\n",
    "        y_shuffled = y[shuffled_indices]\n",
    "        \n",
    "        # 按批次更新参数\n",
    "        for batch_start in range(0, len(X), batch_size):\n",
    "            # 获取当前批次的数据\n",
    "            X_batch = X_shuffled[batch_start:batch_start+batch_size]\n",
    "            y_batch = y_shuffled[batch_start:batch_start+batch_size]\n",
    "            \n",
    "            # 计算梯度\n",
    "            grad = gradient(X_batch, y_batch, weights)\n",
    "            \n",
    "            # 更新参数\n",
    "            weights -= learning_rate * grad\n",
    "            \n",
    "        # 计算当前参数下的目标函数值\n",
    "        cost = logistics_function(X, y, weights)\n",
    "        \n",
    "        # 输出损失\n",
    "        print(\"Epoch: {}, Cost: {}\".format(epoch, cost))\n",
    "    return weights\n",
    "    \n",
    "\n",
    "# # 创建训练数据（示例数据，自行替换）\n",
    "# np.random.seed(0)\n",
    "# X = np.random.randn(1000, 3)\n",
    "# y = np.random.randint(2, size=1000)\n",
    "\n",
    "# # 添加偏置项\n",
    "# X_bias = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "# 运行小批量梯度下降算法\n",
    "mini_batch_gradient_descent(X_bias, y, learning_rate=0.01, batch_size=32, num_epochs=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40,)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_weight = np.random.randn(X.shape[1])\n",
    "standard_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cost: 0.6715693770962268\n",
      "Epoch: 1, Cost: 0.6715660106953437\n",
      "Epoch: 2, Cost: 0.6715912990636204\n",
      "Epoch: 3, Cost: 0.6715958640482066\n",
      "Epoch: 4, Cost: 0.6715950677887222\n",
      "Epoch: 5, Cost: 0.6716051336070907\n",
      "Epoch: 6, Cost: 0.6715816078764367\n",
      "Epoch: 7, Cost: 0.671566334133661\n",
      "Epoch: 8, Cost: 0.671567096587632\n",
      "Epoch: 9, Cost: 0.6717385670894361\n",
      "Epoch: 10, Cost: 0.6716708299014352\n",
      "Epoch: 11, Cost: 0.6715668621545555\n",
      "Epoch: 12, Cost: 0.6719116232507073\n",
      "Epoch: 13, Cost: 0.671751404719541\n",
      "Epoch: 14, Cost: 0.671893059540073\n",
      "Epoch: 15, Cost: 0.6715934674857714\n",
      "Epoch: 16, Cost: 0.6715709644203864\n",
      "Epoch: 17, Cost: 0.6715701458717462\n",
      "Epoch: 18, Cost: 0.6717006861033463\n",
      "Epoch: 19, Cost: 0.6716255261635351\n",
      "Epoch: 20, Cost: 0.6717055663359665\n",
      "Epoch: 21, Cost: 0.6715772303393327\n",
      "Epoch: 22, Cost: 0.6715796022988956\n",
      "Epoch: 23, Cost: 0.6715648899783462\n",
      "Epoch: 24, Cost: 0.6716005365734551\n",
      "Epoch: 25, Cost: 0.6716265887727659\n",
      "Epoch: 26, Cost: 0.671564049648994\n",
      "Epoch: 27, Cost: 0.6717349726739863\n",
      "Epoch: 28, Cost: 0.671589969766937\n",
      "Epoch: 29, Cost: 0.671567193268485\n",
      "Epoch: 30, Cost: 0.6716555806322027\n",
      "Epoch: 31, Cost: 0.671749209375075\n",
      "Epoch: 32, Cost: 0.6715877650570522\n",
      "Epoch: 33, Cost: 0.6715645944679072\n",
      "Epoch: 34, Cost: 0.6715636214760898\n",
      "Epoch: 35, Cost: 0.6715665727359358\n",
      "Epoch: 36, Cost: 0.6715743150995747\n",
      "Epoch: 37, Cost: 0.671591828494687\n",
      "Epoch: 38, Cost: 0.6719102833312344\n",
      "Epoch: 39, Cost: 0.6718650960641119\n",
      "Epoch: 40, Cost: 0.6716314494942057\n",
      "Epoch: 41, Cost: 0.6716903994631073\n",
      "Epoch: 42, Cost: 0.6715672030510038\n",
      "Epoch: 43, Cost: 0.6716063623708216\n",
      "Epoch: 44, Cost: 0.6716139344440003\n",
      "Epoch: 45, Cost: 0.6716870601663972\n",
      "Epoch: 46, Cost: 0.6716937749279416\n",
      "Epoch: 47, Cost: 0.6718462796198802\n",
      "Epoch: 48, Cost: 0.6717125153175529\n",
      "Epoch: 49, Cost: 0.6717689243478698\n",
      "Epoch: 50, Cost: 0.6715670340407737\n",
      "Epoch: 51, Cost: 0.6716507987358807\n",
      "Epoch: 52, Cost: 0.6717552869344935\n",
      "Epoch: 53, Cost: 0.6715638661531296\n",
      "Epoch: 54, Cost: 0.671562248521319\n",
      "Epoch: 55, Cost: 0.6717469125614209\n",
      "Epoch: 56, Cost: 0.6715955625201043\n",
      "Epoch: 57, Cost: 0.6715843825646771\n",
      "Epoch: 58, Cost: 0.671568545436023\n",
      "Epoch: 59, Cost: 0.671565153471146\n",
      "Epoch: 60, Cost: 0.6715612970270863\n",
      "Epoch: 61, Cost: 0.6715601378099105\n",
      "Epoch: 62, Cost: 0.671560180185038\n",
      "Epoch: 63, Cost: 0.6715861535405317\n",
      "Epoch: 64, Cost: 0.6715623076492907\n",
      "Epoch: 65, Cost: 0.6715604113686771\n",
      "Epoch: 66, Cost: 0.6715685164616662\n",
      "Epoch: 67, Cost: 0.6716288067004279\n",
      "Epoch: 68, Cost: 0.671644968867563\n",
      "Epoch: 69, Cost: 0.6718394019263422\n",
      "Epoch: 70, Cost: 0.6715798951892689\n",
      "Epoch: 71, Cost: 0.6715711287882137\n",
      "Epoch: 72, Cost: 0.6715607115046046\n",
      "Epoch: 73, Cost: 0.6715618220363376\n",
      "Epoch: 74, Cost: 0.6715600074776632\n",
      "Epoch: 75, Cost: 0.6715599624985029\n",
      "Epoch: 76, Cost: 0.6716405090928863\n",
      "Epoch: 77, Cost: 0.671646328308981\n",
      "Epoch: 78, Cost: 0.6716111765614751\n",
      "Epoch: 79, Cost: 0.6716541196081071\n",
      "Epoch: 80, Cost: 0.6715719763774087\n",
      "Epoch: 81, Cost: 0.6716026697503996\n",
      "Epoch: 82, Cost: 0.6715952155123288\n",
      "Epoch: 83, Cost: 0.671573309854921\n",
      "Epoch: 84, Cost: 0.6715922540638747\n",
      "Epoch: 85, Cost: 0.6715588639213438\n",
      "Epoch: 86, Cost: 0.6715707912792166\n",
      "Epoch: 87, Cost: 0.6716048888934261\n",
      "Epoch: 88, Cost: 0.6716809221843147\n",
      "Epoch: 89, Cost: 0.671572819557658\n",
      "Epoch: 90, Cost: 0.6715652675442908\n",
      "Epoch: 91, Cost: 0.6715975565310687\n",
      "Epoch: 92, Cost: 0.6715836269969174\n",
      "Epoch: 93, Cost: 0.6715761588745535\n",
      "Epoch: 94, Cost: 0.6715684218234487\n",
      "Epoch: 95, Cost: 0.6716807255642018\n",
      "Epoch: 96, Cost: 0.6716748761580454\n",
      "Epoch: 97, Cost: 0.6716456578517489\n",
      "Epoch: 98, Cost: 0.6716661718415384\n",
      "Epoch: 99, Cost: 0.6715801027812688\n",
      "Epoch: 100, Cost: 0.6715679188959837\n",
      "Epoch: 101, Cost: 0.6718671335814795\n",
      "Epoch: 102, Cost: 0.6715753615900101\n",
      "Epoch: 103, Cost: 0.6716618882388402\n",
      "Epoch: 104, Cost: 0.6715755557666075\n",
      "Epoch: 105, Cost: 0.671577221609984\n",
      "Epoch: 106, Cost: 0.6715866843642807\n",
      "Epoch: 107, Cost: 0.6715638005243859\n",
      "Epoch: 108, Cost: 0.6715901156061682\n",
      "Epoch: 109, Cost: 0.6715597219186518\n",
      "Epoch: 110, Cost: 0.6715797697766672\n",
      "Epoch: 111, Cost: 0.6715570415543263\n",
      "Epoch: 112, Cost: 0.6715567903444172\n",
      "Epoch: 113, Cost: 0.6715576816046958\n",
      "Epoch: 114, Cost: 0.6715807021815808\n",
      "Epoch: 115, Cost: 0.6715794551412173\n",
      "Epoch: 116, Cost: 0.6715593625712664\n",
      "Epoch: 117, Cost: 0.6715566944932008\n",
      "Epoch: 118, Cost: 0.6715635034066466\n",
      "Epoch: 119, Cost: 0.6715604521487276\n",
      "Epoch: 120, Cost: 0.6715688168308928\n",
      "Epoch: 121, Cost: 0.671607067865122\n",
      "Epoch: 122, Cost: 0.6715561194274032\n",
      "Epoch: 123, Cost: 0.6715659007410532\n",
      "Epoch: 124, Cost: 0.6716883940986841\n",
      "Epoch: 125, Cost: 0.6716502204320772\n",
      "Epoch: 126, Cost: 0.6715839402644498\n",
      "Epoch: 127, Cost: 0.6715565104661753\n",
      "Epoch: 128, Cost: 0.6715580051070303\n",
      "Epoch: 129, Cost: 0.671654834448432\n",
      "Epoch: 130, Cost: 0.6715932729127976\n",
      "Epoch: 131, Cost: 0.6717311631200735\n",
      "Epoch: 132, Cost: 0.6715682658312246\n",
      "Epoch: 133, Cost: 0.6716149488461383\n",
      "Epoch: 134, Cost: 0.6715838402979876\n",
      "Epoch: 135, Cost: 0.6716302939554823\n",
      "Epoch: 136, Cost: 0.6715784812451352\n",
      "Epoch: 137, Cost: 0.6716302565968651\n",
      "Epoch: 138, Cost: 0.6716320694151272\n",
      "Epoch: 139, Cost: 0.6715725773062509\n",
      "Epoch: 140, Cost: 0.6715562102857762\n",
      "Epoch: 141, Cost: 0.6716276189859841\n",
      "Epoch: 142, Cost: 0.6715615533197333\n",
      "Epoch: 143, Cost: 0.6715781842022461\n",
      "Epoch: 144, Cost: 0.6715702153671157\n",
      "Epoch: 145, Cost: 0.6715969086131849\n",
      "Epoch: 146, Cost: 0.6715654653438737\n",
      "Epoch: 147, Cost: 0.6715659974090081\n",
      "Epoch: 148, Cost: 0.67157643973675\n",
      "Epoch: 149, Cost: 0.6715609332248276\n",
      "Epoch: 150, Cost: 0.6715576994020472\n",
      "Epoch: 151, Cost: 0.6716490970577593\n",
      "Epoch: 152, Cost: 0.6715736970458557\n",
      "Epoch: 153, Cost: 0.6717041201735728\n",
      "Epoch: 154, Cost: 0.671686929407198\n",
      "Epoch: 155, Cost: 0.6715555223490776\n",
      "Epoch: 156, Cost: 0.6715565069960754\n",
      "Epoch: 157, Cost: 0.6715727937155928\n",
      "Epoch: 158, Cost: 0.6715832846519164\n",
      "Epoch: 159, Cost: 0.6715561851345564\n",
      "Epoch: 160, Cost: 0.6716519339747602\n",
      "Epoch: 161, Cost: 0.6715986028072446\n",
      "Epoch: 162, Cost: 0.671554772002881\n",
      "Epoch: 163, Cost: 0.6715551767047245\n",
      "Epoch: 164, Cost: 0.6715909027471075\n",
      "Epoch: 165, Cost: 0.6715639257057944\n",
      "Epoch: 166, Cost: 0.6716696185852761\n",
      "Epoch: 167, Cost: 0.6715558200853308\n",
      "Epoch: 168, Cost: 0.6715563198672835\n",
      "Epoch: 169, Cost: 0.6716178605176398\n",
      "Epoch: 170, Cost: 0.6715743601969216\n",
      "Epoch: 171, Cost: 0.6715693051853777\n",
      "Epoch: 172, Cost: 0.6715602100235858\n",
      "Epoch: 173, Cost: 0.6715552810052482\n",
      "Epoch: 174, Cost: 0.6715547762251296\n",
      "Epoch: 175, Cost: 0.6716239957443717\n",
      "Epoch: 176, Cost: 0.6716231104278061\n",
      "Epoch: 177, Cost: 0.6715560760017106\n",
      "Epoch: 178, Cost: 0.6715567904213637\n",
      "Epoch: 179, Cost: 0.6715722971983558\n",
      "Epoch: 180, Cost: 0.6715678764058927\n",
      "Epoch: 181, Cost: 0.6715547109349492\n",
      "Epoch: 182, Cost: 0.671553393123987\n",
      "Epoch: 183, Cost: 0.6715997538871575\n",
      "Epoch: 184, Cost: 0.6715596929586017\n",
      "Epoch: 185, Cost: 0.6715755676285333\n",
      "Epoch: 186, Cost: 0.6715546368739755\n",
      "Epoch: 187, Cost: 0.6715671011146341\n",
      "Epoch: 188, Cost: 0.6715743447791028\n",
      "Epoch: 189, Cost: 0.6715523150844126\n",
      "Epoch: 190, Cost: 0.6715564747766021\n",
      "Epoch: 191, Cost: 0.6715743651849675\n",
      "Epoch: 192, Cost: 0.6715751150229377\n",
      "Epoch: 193, Cost: 0.671567091167157\n",
      "Epoch: 194, Cost: 0.6715824274791206\n",
      "Epoch: 195, Cost: 0.6716252772946152\n",
      "Epoch: 196, Cost: 0.671625060577703\n",
      "Epoch: 197, Cost: 0.6715573824797104\n",
      "Epoch: 198, Cost: 0.6716648908505268\n",
      "Epoch: 199, Cost: 0.671580302052411\n",
      "Epoch: 200, Cost: 0.6717014147836328\n",
      "Epoch: 201, Cost: 0.6715787318344324\n",
      "Epoch: 202, Cost: 0.671585903415225\n",
      "Epoch: 203, Cost: 0.6715819281830978\n",
      "Epoch: 204, Cost: 0.6715736278026595\n",
      "Epoch: 205, Cost: 0.6716249737178128\n",
      "Epoch: 206, Cost: 0.6715911123844874\n",
      "Epoch: 207, Cost: 0.6716244630803281\n",
      "Epoch: 208, Cost: 0.6716100721686273\n",
      "Epoch: 209, Cost: 0.6715813328643688\n",
      "Epoch: 210, Cost: 0.6716590422153128\n",
      "Epoch: 211, Cost: 0.6715508062334562\n",
      "Epoch: 212, Cost: 0.6715515182611262\n",
      "Epoch: 213, Cost: 0.6716290054686114\n",
      "Epoch: 214, Cost: 0.671553818827231\n",
      "Epoch: 215, Cost: 0.6716647744472574\n",
      "Epoch: 216, Cost: 0.6715708548636833\n",
      "Epoch: 217, Cost: 0.6715549746763095\n",
      "Epoch: 218, Cost: 0.6715756333885715\n",
      "Epoch: 219, Cost: 0.6719216900623651\n",
      "Epoch: 220, Cost: 0.6716825206273884\n",
      "Epoch: 221, Cost: 0.6715522408534539\n",
      "Epoch: 222, Cost: 0.6715607024253902\n",
      "Epoch: 223, Cost: 0.6716627979030655\n",
      "Epoch: 224, Cost: 0.6715694678109971\n",
      "Epoch: 225, Cost: 0.6716291830121123\n",
      "Epoch: 226, Cost: 0.6716127767350281\n",
      "Epoch: 227, Cost: 0.6715760773180839\n",
      "Epoch: 228, Cost: 0.6715714972159774\n",
      "Epoch: 229, Cost: 0.6715620210611211\n",
      "Epoch: 230, Cost: 0.671596847207864\n",
      "Epoch: 231, Cost: 0.6715507367127459\n",
      "Epoch: 232, Cost: 0.6715530874767105\n",
      "Epoch: 233, Cost: 0.6715560136177776\n",
      "Epoch: 234, Cost: 0.6716135733246472\n",
      "Epoch: 235, Cost: 0.671619465499428\n",
      "Epoch: 236, Cost: 0.6715850004550775\n",
      "Epoch: 237, Cost: 0.6716784524231345\n",
      "Epoch: 238, Cost: 0.6715503410737725\n",
      "Epoch: 239, Cost: 0.6715511341291048\n",
      "Epoch: 240, Cost: 0.671555126582299\n",
      "Epoch: 241, Cost: 0.6715503374491681\n",
      "Epoch: 242, Cost: 0.6716456670293401\n",
      "Epoch: 243, Cost: 0.6715999006875639\n",
      "Epoch: 244, Cost: 0.6715601285038789\n",
      "Epoch: 245, Cost: 0.6715506030051202\n",
      "Epoch: 246, Cost: 0.6715496898209825\n",
      "Epoch: 247, Cost: 0.6715586817272445\n",
      "Epoch: 248, Cost: 0.6715497052916212\n",
      "Epoch: 249, Cost: 0.671555524145604\n",
      "Epoch: 250, Cost: 0.6716176806552413\n",
      "Epoch: 251, Cost: 0.6715515694417701\n",
      "Epoch: 252, Cost: 0.6715888924990565\n",
      "Epoch: 253, Cost: 0.6716262978529616\n",
      "Epoch: 254, Cost: 0.671596385642636\n",
      "Epoch: 255, Cost: 0.6715593946105969\n",
      "Epoch: 256, Cost: 0.6716252725984507\n",
      "Epoch: 257, Cost: 0.6716829095993858\n",
      "Epoch: 258, Cost: 0.6716349492389502\n",
      "Epoch: 259, Cost: 0.6716470108730973\n",
      "Epoch: 260, Cost: 0.6715563249193134\n",
      "Epoch: 261, Cost: 0.6715783291817585\n",
      "Epoch: 262, Cost: 0.671558758151988\n",
      "Epoch: 263, Cost: 0.6715642004317202\n",
      "Epoch: 264, Cost: 0.6715715492140367\n",
      "Epoch: 265, Cost: 0.6715487244218467\n",
      "Epoch: 266, Cost: 0.6716371569254352\n",
      "Epoch: 267, Cost: 0.6715506198319339\n",
      "Epoch: 268, Cost: 0.6715880550713346\n",
      "Epoch: 269, Cost: 0.6716767924257754\n",
      "Epoch: 270, Cost: 0.6715929106244691\n",
      "Epoch: 271, Cost: 0.6715513185879336\n",
      "Epoch: 272, Cost: 0.6715704012919389\n",
      "Epoch: 273, Cost: 0.6716059788202258\n",
      "Epoch: 274, Cost: 0.6716235913458919\n",
      "Epoch: 275, Cost: 0.6715550017431108\n",
      "Epoch: 276, Cost: 0.6715572590029988\n",
      "Epoch: 277, Cost: 0.671566551903186\n",
      "Epoch: 278, Cost: 0.6715773840717729\n",
      "Epoch: 279, Cost: 0.6715607174065262\n",
      "Epoch: 280, Cost: 0.6715529460625901\n",
      "Epoch: 281, Cost: 0.6715487103515067\n",
      "Epoch: 282, Cost: 0.6715513640448689\n",
      "Epoch: 283, Cost: 0.6715595192443533\n",
      "Epoch: 284, Cost: 0.6715555919103379\n",
      "Epoch: 285, Cost: 0.6715842693327508\n",
      "Epoch: 286, Cost: 0.6716372562837211\n",
      "Epoch: 287, Cost: 0.6715996197725527\n",
      "Epoch: 288, Cost: 0.6715623834809349\n",
      "Epoch: 289, Cost: 0.671546632150965\n",
      "Epoch: 290, Cost: 0.6715877562424136\n",
      "Epoch: 291, Cost: 0.6716370020575009\n",
      "Epoch: 292, Cost: 0.6716240066151432\n",
      "Epoch: 293, Cost: 0.6715482945433693\n",
      "Epoch: 294, Cost: 0.6715471900596319\n",
      "Epoch: 295, Cost: 0.6715467305891971\n",
      "Epoch: 296, Cost: 0.6715462235492078\n",
      "Epoch: 297, Cost: 0.6715623360744394\n",
      "Epoch: 298, Cost: 0.6715585870047898\n",
      "Epoch: 299, Cost: 0.6715624152319709\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.55097321, -0.01073398, -0.06475282,  0.11195287, -0.17989966,\n",
       "       -0.33710224, -0.12955031, -0.53813607, -0.44203356,  0.08015904,\n",
       "        0.3003148 , -0.27260057,  0.37144702, -0.04545986, -0.22080118,\n",
       "       -0.35641876,  0.15124222,  0.05201362,  0.04645895, -0.26709189,\n",
       "        0.22615389, -0.23540706,  0.05210057, -0.13346274,  0.19189372,\n",
       "       -0.15522687, -0.20296934, -0.09332131,  0.42972255,  0.13704143,\n",
       "        0.1398835 ,  0.14624136,  0.16335998,  0.05637193,  0.15886293,\n",
       "        0.1094274 ,  0.23154241,  0.03389491, -0.59832433, -0.11905291])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 定义目标函数\n",
    "def loss_function(X, y, weights):\n",
    "    z = np.dot(X, weights)\n",
    "    predictions = 1 / (1 + np.exp(-z))\n",
    "    loss = -np.mean(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))\n",
    "    return loss\n",
    "\n",
    "# 计算目标函数关于参数的梯度\n",
    "def gradient(X, y, weights):\n",
    "    z = np.dot(X, weights)\n",
    "    predictions = 1 / (1 + np.exp(-z))\n",
    "    error = predictions - y\n",
    "    grad = np.dot(X.T, error) / len(X)\n",
    "    return grad\n",
    "\n",
    "# KL散度的计算函数\n",
    "# def kl_divergence(params1, params2):\n",
    "#     # 根据模型参数计算概率分布\n",
    "#     # ...\n",
    "\n",
    "#     # 计算KL散度\n",
    "#     # kl = ...\n",
    "#     kl = 0  # 这里需要根据具体的模型参数和KL散度的定义进行计算\n",
    "#     return kl\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x)\n",
    "    # 使用Softmax函数将旧模型参数映射到概率分布\n",
    "\n",
    "\n",
    "def kl_divergence(params1,params2):\n",
    " \n",
    "   # 计算离散概率分布的KL散度\n",
    "    oldparams_prob = softmax(params1)\n",
    "    batchparams_prob = softmax(params2)\n",
    "    kl = np.sum(np.array(oldparams_prob) * np.log(np.array(oldparams_prob)/np.array(batchparams_prob)))\n",
    "    return kl\n",
    "\n",
    "# 小批量梯度下降算法\n",
    "def mini_batch_gradient_descent(X, y, learning_rate=0.001, batch_size=32, num_epochs=100, regularization_weight=0.1):\n",
    "    # 初始化参数\n",
    "    # weights = np.random.randn(X.shape[1])\n",
    "    weights= standard_weight\n",
    "    # 保存上一次的参数\n",
    "    prev_weights = weights.copy()\n",
    "    \n",
    "    # 迭代训练\n",
    "    for epoch in range(num_epochs):\n",
    "        # 在每个epoch开始时，对训练数据进行洗牌\n",
    "        shuffled_indices = np.random.permutation(len(X))\n",
    "        X_shuffled = X[shuffled_indices]\n",
    "        y_shuffled = y[shuffled_indices]\n",
    "        \n",
    "        # 按批次更新参数\n",
    "        for batch_start in range(0, len(X), batch_size):\n",
    "            # 获取当前批次的数据\n",
    "            X_batch = X_shuffled[batch_start:batch_start+batch_size]\n",
    "            y_batch = y_shuffled[batch_start:batch_start+batch_size]\n",
    "            \n",
    "            # 计算梯度\n",
    "            grad = gradient(X_batch, y_batch, weights)\n",
    "            \n",
    "            # 计算KL散度\n",
    "            kl = kl_divergence(weights, prev_weights)\n",
    "            \n",
    "            # 更新参数\n",
    "            weights -= learning_rate * grad + regularization_weight * kl\n",
    "            \n",
    "            # 更新上一次的参数\n",
    "            prev_weights = weights.copy()\n",
    "            \n",
    "        # 计算当前参数下的目标函数值\n",
    "        cost = loss_function(X, y, weights)\n",
    "        \n",
    "        # 输出损失\n",
    "        print(\"Epoch: {}, Cost: {}\".format(epoch, cost))\n",
    "    return weights\n",
    "# 创建训练数据（示例数据，自行替换）\n",
    "np.random.seed(0)\n",
    "# X = np.random.randn(1000, 3)\n",
    "X= np.random.random(size=(1000,39))\n",
    "y = np.random.randint(2, size=1000)\n",
    "\n",
    "# 添加偏置项\n",
    "X_bias = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "# 运行小批量梯度下降算法\n",
    "mini_batch_gradient_descent(X_bias, y, learning_rate=0.01, batch_size=32, num_epochs=300, regularization_weight=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cost: 0.6716188921350539\n",
      "Epoch: 1, Cost: 0.6716281895996586\n",
      "Epoch: 2, Cost: 0.6715966152601636\n",
      "Epoch: 3, Cost: 0.6715969385656927\n",
      "Epoch: 4, Cost: 0.6715962115402176\n",
      "Epoch: 5, Cost: 0.6715966806045798\n",
      "Epoch: 6, Cost: 0.6716440598134604\n",
      "Epoch: 7, Cost: 0.6716607159330591\n",
      "Epoch: 8, Cost: 0.6717224625710937\n",
      "Epoch: 9, Cost: 0.6717471803665953\n",
      "Epoch: 10, Cost: 0.6718583751546048\n",
      "Epoch: 11, Cost: 0.6716525243080544\n",
      "Epoch: 12, Cost: 0.6715971191552836\n",
      "Epoch: 13, Cost: 0.6717713725795598\n",
      "Epoch: 14, Cost: 0.671724635783806\n",
      "Epoch: 15, Cost: 0.6716139894608296\n",
      "Epoch: 16, Cost: 0.6716498030448759\n",
      "Epoch: 17, Cost: 0.6716001481246814\n",
      "Epoch: 18, Cost: 0.6715951951420216\n",
      "Epoch: 19, Cost: 0.6716690308074224\n",
      "Epoch: 20, Cost: 0.6716043027432357\n",
      "Epoch: 21, Cost: 0.6715963358534816\n",
      "Epoch: 22, Cost: 0.6717204094905465\n",
      "Epoch: 23, Cost: 0.6716508458170144\n",
      "Epoch: 24, Cost: 0.6718455751682368\n",
      "Epoch: 25, Cost: 0.6717917071796096\n",
      "Epoch: 26, Cost: 0.6716055815552215\n",
      "Epoch: 27, Cost: 0.6716321807399638\n",
      "Epoch: 28, Cost: 0.6716027099876347\n",
      "Epoch: 29, Cost: 0.6717126938343596\n",
      "Epoch: 30, Cost: 0.6716925240619637\n",
      "Epoch: 31, Cost: 0.6716038824332377\n",
      "Epoch: 32, Cost: 0.6715935461964848\n",
      "Epoch: 33, Cost: 0.6716189094767322\n",
      "Epoch: 34, Cost: 0.6716393532390393\n",
      "Epoch: 35, Cost: 0.6716141211517968\n",
      "Epoch: 36, Cost: 0.6716154963957469\n",
      "Epoch: 37, Cost: 0.671598603651088\n",
      "Epoch: 38, Cost: 0.6716010730656171\n",
      "Epoch: 39, Cost: 0.6716141310051266\n",
      "Epoch: 40, Cost: 0.6716044456431719\n",
      "Epoch: 41, Cost: 0.6715921909909705\n",
      "Epoch: 42, Cost: 0.6715972756055218\n",
      "Epoch: 43, Cost: 0.6715971412463431\n",
      "Epoch: 44, Cost: 0.6716196498305969\n",
      "Epoch: 45, Cost: 0.6716022751619526\n",
      "Epoch: 46, Cost: 0.6715977612683369\n",
      "Epoch: 47, Cost: 0.6715919657261379\n",
      "Epoch: 48, Cost: 0.6715903784569346\n",
      "Epoch: 49, Cost: 0.671612246738511\n",
      "Epoch: 50, Cost: 0.6716031355430374\n",
      "Epoch: 51, Cost: 0.6716852842948724\n",
      "Epoch: 52, Cost: 0.6715908883020584\n",
      "Epoch: 53, Cost: 0.67159194800407\n",
      "Epoch: 54, Cost: 0.6715938697545221\n",
      "Epoch: 55, Cost: 0.6715933331801393\n",
      "Epoch: 56, Cost: 0.6715919547066258\n",
      "Epoch: 57, Cost: 0.6715902359583138\n",
      "Epoch: 58, Cost: 0.6716408557081357\n",
      "Epoch: 59, Cost: 0.6716740658728317\n",
      "Epoch: 60, Cost: 0.6716459931730416\n",
      "Epoch: 61, Cost: 0.6717014082014063\n",
      "Epoch: 62, Cost: 0.6716066938862288\n",
      "Epoch: 63, Cost: 0.6715903448084305\n",
      "Epoch: 64, Cost: 0.6715967415308199\n",
      "Epoch: 65, Cost: 0.6715918513888817\n",
      "Epoch: 66, Cost: 0.6715874610266298\n",
      "Epoch: 67, Cost: 0.6715904353137633\n",
      "Epoch: 68, Cost: 0.6716353127985861\n",
      "Epoch: 69, Cost: 0.6715889110549583\n",
      "Epoch: 70, Cost: 0.6715865833584281\n",
      "Epoch: 71, Cost: 0.6715897974415366\n",
      "Epoch: 72, Cost: 0.6716512308272758\n",
      "Epoch: 73, Cost: 0.6716331601741068\n",
      "Epoch: 74, Cost: 0.6715957591533805\n",
      "Epoch: 75, Cost: 0.6715861694923154\n",
      "Epoch: 76, Cost: 0.6715861931791745\n",
      "Epoch: 77, Cost: 0.6715870372285705\n",
      "Epoch: 78, Cost: 0.6715979980966548\n",
      "Epoch: 79, Cost: 0.6715885580700679\n",
      "Epoch: 80, Cost: 0.6716575654452943\n",
      "Epoch: 81, Cost: 0.671588927853307\n",
      "Epoch: 82, Cost: 0.6715869941861707\n",
      "Epoch: 83, Cost: 0.6716297495634506\n",
      "Epoch: 84, Cost: 0.6715855121067418\n",
      "Epoch: 85, Cost: 0.6716050093416982\n",
      "Epoch: 86, Cost: 0.6715964034082746\n",
      "Epoch: 87, Cost: 0.6715897952350454\n",
      "Epoch: 88, Cost: 0.6716319478389814\n",
      "Epoch: 89, Cost: 0.6716288979125814\n",
      "Epoch: 90, Cost: 0.6715950810411404\n",
      "Epoch: 91, Cost: 0.6715836956272938\n",
      "Epoch: 92, Cost: 0.6715837018118233\n",
      "Epoch: 93, Cost: 0.671607254339825\n",
      "Epoch: 94, Cost: 0.6716113564535674\n",
      "Epoch: 95, Cost: 0.6715930482989179\n",
      "Epoch: 96, Cost: 0.6715885206559312\n",
      "Epoch: 97, Cost: 0.671608963699764\n",
      "Epoch: 98, Cost: 0.6716100324836292\n",
      "Epoch: 99, Cost: 0.6716566317966111\n",
      "Epoch: 100, Cost: 0.6716029551700937\n",
      "Epoch: 101, Cost: 0.6716416877727798\n",
      "Epoch: 102, Cost: 0.671586418261617\n",
      "Epoch: 103, Cost: 0.6715986005441786\n",
      "Epoch: 104, Cost: 0.6715857507167105\n",
      "Epoch: 105, Cost: 0.6716346839797498\n",
      "Epoch: 106, Cost: 0.6716892015762365\n",
      "Epoch: 107, Cost: 0.6715830768510198\n",
      "Epoch: 108, Cost: 0.6715835476247385\n",
      "Epoch: 109, Cost: 0.6716176539510484\n",
      "Epoch: 110, Cost: 0.6717577409818564\n",
      "Epoch: 111, Cost: 0.6715829629874597\n",
      "Epoch: 112, Cost: 0.6715899823631964\n",
      "Epoch: 113, Cost: 0.6716391452600099\n",
      "Epoch: 114, Cost: 0.671582572499685\n",
      "Epoch: 115, Cost: 0.6715823953615719\n",
      "Epoch: 116, Cost: 0.6715950409306138\n",
      "Epoch: 117, Cost: 0.6715835513414506\n",
      "Epoch: 118, Cost: 0.6717075472967686\n",
      "Epoch: 119, Cost: 0.6716713857858808\n",
      "Epoch: 120, Cost: 0.6716690130279142\n",
      "Epoch: 121, Cost: 0.6716123540418557\n",
      "Epoch: 122, Cost: 0.6715904481410057\n",
      "Epoch: 123, Cost: 0.6715854487520244\n",
      "Epoch: 124, Cost: 0.6718539352675642\n",
      "Epoch: 125, Cost: 0.671653016660947\n",
      "Epoch: 126, Cost: 0.6716876982501874\n",
      "Epoch: 127, Cost: 0.6716026130488439\n",
      "Epoch: 128, Cost: 0.6715923478940984\n",
      "Epoch: 129, Cost: 0.6716985429841739\n",
      "Epoch: 130, Cost: 0.6716090688750357\n",
      "Epoch: 131, Cost: 0.6715870462397234\n",
      "Epoch: 132, Cost: 0.6717692732581492\n",
      "Epoch: 133, Cost: 0.6717128143631905\n",
      "Epoch: 134, Cost: 0.6716392901254453\n",
      "Epoch: 135, Cost: 0.6715810420773182\n",
      "Epoch: 136, Cost: 0.6715869730442269\n",
      "Epoch: 137, Cost: 0.6716473867213792\n",
      "Epoch: 138, Cost: 0.6716095120536254\n",
      "Epoch: 139, Cost: 0.6715813612123663\n",
      "Epoch: 140, Cost: 0.6716225436691187\n",
      "Epoch: 141, Cost: 0.6716228133358076\n",
      "Epoch: 142, Cost: 0.6716054165076999\n",
      "Epoch: 143, Cost: 0.6716197559571822\n",
      "Epoch: 144, Cost: 0.6715797777488269\n",
      "Epoch: 145, Cost: 0.6716193458579719\n",
      "Epoch: 146, Cost: 0.6716205520210865\n",
      "Epoch: 147, Cost: 0.6716689966223098\n",
      "Epoch: 148, Cost: 0.6719355895175484\n",
      "Epoch: 149, Cost: 0.6716283393858528\n",
      "Epoch: 150, Cost: 0.6715897662752782\n",
      "Epoch: 151, Cost: 0.6715811458891269\n",
      "Epoch: 152, Cost: 0.6716401085183575\n",
      "Epoch: 153, Cost: 0.6718077307717765\n",
      "Epoch: 154, Cost: 0.6716407851795766\n",
      "Epoch: 155, Cost: 0.671595254336333\n",
      "Epoch: 156, Cost: 0.6715933772230183\n",
      "Epoch: 157, Cost: 0.6715827102713958\n",
      "Epoch: 158, Cost: 0.6715846275466337\n",
      "Epoch: 159, Cost: 0.6716427126170722\n",
      "Epoch: 160, Cost: 0.6715869711429022\n",
      "Epoch: 161, Cost: 0.6716518048692143\n",
      "Epoch: 162, Cost: 0.6715878589853949\n",
      "Epoch: 163, Cost: 0.6716289305189129\n",
      "Epoch: 164, Cost: 0.6715806919670052\n",
      "Epoch: 165, Cost: 0.6715977903305715\n",
      "Epoch: 166, Cost: 0.6715913163093289\n",
      "Epoch: 167, Cost: 0.6715778931266401\n",
      "Epoch: 168, Cost: 0.6715816842563876\n",
      "Epoch: 169, Cost: 0.6715830148339295\n",
      "Epoch: 170, Cost: 0.6716302352681478\n",
      "Epoch: 171, Cost: 0.671627636858491\n",
      "Epoch: 172, Cost: 0.6715771740081864\n",
      "Epoch: 173, Cost: 0.6715926905521844\n",
      "Epoch: 174, Cost: 0.6715810315311129\n",
      "Epoch: 175, Cost: 0.6715885958923024\n",
      "Epoch: 176, Cost: 0.6716276837739403\n",
      "Epoch: 177, Cost: 0.671576681927341\n",
      "Epoch: 178, Cost: 0.6716493592594017\n",
      "Epoch: 179, Cost: 0.6715819650437305\n",
      "Epoch: 180, Cost: 0.6715768487117572\n",
      "Epoch: 181, Cost: 0.6717014672637472\n",
      "Epoch: 182, Cost: 0.6715766816786763\n",
      "Epoch: 183, Cost: 0.6717048434019769\n",
      "Epoch: 184, Cost: 0.6716299184325509\n",
      "Epoch: 185, Cost: 0.6715922129303695\n",
      "Epoch: 186, Cost: 0.6715844769420993\n",
      "Epoch: 187, Cost: 0.6715776042286954\n",
      "Epoch: 188, Cost: 0.6715748584871178\n",
      "Epoch: 189, Cost: 0.6715756242392132\n",
      "Epoch: 190, Cost: 0.671589553190128\n",
      "Epoch: 191, Cost: 0.6717249059186965\n",
      "Epoch: 192, Cost: 0.671607360742776\n",
      "Epoch: 193, Cost: 0.6715740053796483\n",
      "Epoch: 194, Cost: 0.671575090968147\n",
      "Epoch: 195, Cost: 0.6715808221382751\n",
      "Epoch: 196, Cost: 0.6715933968065124\n",
      "Epoch: 197, Cost: 0.6716902344633099\n",
      "Epoch: 198, Cost: 0.6716052439750981\n",
      "Epoch: 199, Cost: 0.6715785459725212\n",
      "Epoch: 200, Cost: 0.6715936117406256\n",
      "Epoch: 201, Cost: 0.6715956366416187\n",
      "Epoch: 202, Cost: 0.6715736885574217\n",
      "Epoch: 203, Cost: 0.6715776150186132\n",
      "Epoch: 204, Cost: 0.6716678816399938\n",
      "Epoch: 205, Cost: 0.6715731286361833\n",
      "Epoch: 206, Cost: 0.6718604893951735\n",
      "Epoch: 207, Cost: 0.6716089130760263\n",
      "Epoch: 208, Cost: 0.6716179307007633\n",
      "Epoch: 209, Cost: 0.6715776683053426\n",
      "Epoch: 210, Cost: 0.6716047215834245\n",
      "Epoch: 211, Cost: 0.6716049916076676\n",
      "Epoch: 212, Cost: 0.6716040324112718\n",
      "Epoch: 213, Cost: 0.6715793094133298\n",
      "Epoch: 214, Cost: 0.671606920063024\n",
      "Epoch: 215, Cost: 0.6715798145714172\n",
      "Epoch: 216, Cost: 0.6716671619063856\n",
      "Epoch: 217, Cost: 0.6716472087486851\n",
      "Epoch: 218, Cost: 0.6715723609762588\n",
      "Epoch: 219, Cost: 0.6715720582322774\n",
      "Epoch: 220, Cost: 0.6715930173298406\n",
      "Epoch: 221, Cost: 0.6716472969418733\n",
      "Epoch: 222, Cost: 0.671572384694357\n",
      "Epoch: 223, Cost: 0.6717167111098149\n",
      "Epoch: 224, Cost: 0.6716616115932476\n",
      "Epoch: 225, Cost: 0.6715717449652363\n",
      "Epoch: 226, Cost: 0.6715972564216176\n",
      "Epoch: 227, Cost: 0.6715751314310505\n",
      "Epoch: 228, Cost: 0.6715724377894048\n",
      "Epoch: 229, Cost: 0.6715734447760681\n",
      "Epoch: 230, Cost: 0.6715814565178518\n",
      "Epoch: 231, Cost: 0.671575156296919\n",
      "Epoch: 232, Cost: 0.6716039369292106\n",
      "Epoch: 233, Cost: 0.6716619203816294\n",
      "Epoch: 234, Cost: 0.6715863531457766\n",
      "Epoch: 235, Cost: 0.6715824963611255\n",
      "Epoch: 236, Cost: 0.6716134482561422\n",
      "Epoch: 237, Cost: 0.6715749932304507\n",
      "Epoch: 238, Cost: 0.6715807928653189\n",
      "Epoch: 239, Cost: 0.6715752755221608\n",
      "Epoch: 240, Cost: 0.6715727327117097\n",
      "Epoch: 241, Cost: 0.6715931783192748\n",
      "Epoch: 242, Cost: 0.6715824441094553\n",
      "Epoch: 243, Cost: 0.6715788286285785\n",
      "Epoch: 244, Cost: 0.6715793677412056\n",
      "Epoch: 245, Cost: 0.6715723926393002\n",
      "Epoch: 246, Cost: 0.6715715663931171\n",
      "Epoch: 247, Cost: 0.6715898539336906\n",
      "Epoch: 248, Cost: 0.6716301162379589\n",
      "Epoch: 249, Cost: 0.6715704480874918\n",
      "Epoch: 250, Cost: 0.6715717578101703\n",
      "Epoch: 251, Cost: 0.671667487003419\n",
      "Epoch: 252, Cost: 0.6716112149828861\n",
      "Epoch: 253, Cost: 0.6717193154105983\n",
      "Epoch: 254, Cost: 0.6717319039507891\n",
      "Epoch: 255, Cost: 0.6715741054421648\n",
      "Epoch: 256, Cost: 0.6715774807454464\n",
      "Epoch: 257, Cost: 0.6715703210987549\n",
      "Epoch: 258, Cost: 0.671667378473225\n",
      "Epoch: 259, Cost: 0.6715777808584792\n",
      "Epoch: 260, Cost: 0.6715689969445336\n",
      "Epoch: 261, Cost: 0.6716328125930696\n",
      "Epoch: 262, Cost: 0.671579117947399\n",
      "Epoch: 263, Cost: 0.6715743513106351\n",
      "Epoch: 264, Cost: 0.6715713345552393\n",
      "Epoch: 265, Cost: 0.6716279238293141\n",
      "Epoch: 266, Cost: 0.6716344785273032\n",
      "Epoch: 267, Cost: 0.671569336984104\n",
      "Epoch: 268, Cost: 0.6716023178621238\n",
      "Epoch: 269, Cost: 0.6719032901359779\n",
      "Epoch: 270, Cost: 0.6717547944225409\n",
      "Epoch: 271, Cost: 0.6716121703835022\n",
      "Epoch: 272, Cost: 0.6716275642031078\n",
      "Epoch: 273, Cost: 0.6716282854965814\n",
      "Epoch: 274, Cost: 0.6715704677190092\n",
      "Epoch: 275, Cost: 0.6716048394536865\n",
      "Epoch: 276, Cost: 0.6715689613719976\n",
      "Epoch: 277, Cost: 0.6715782283597016\n",
      "Epoch: 278, Cost: 0.6716607427508896\n",
      "Epoch: 279, Cost: 0.6715709865163492\n",
      "Epoch: 280, Cost: 0.6716772316231565\n",
      "Epoch: 281, Cost: 0.6715840507410877\n",
      "Epoch: 282, Cost: 0.6715684866088614\n",
      "Epoch: 283, Cost: 0.6715728146613931\n",
      "Epoch: 284, Cost: 0.6718261711692979\n",
      "Epoch: 285, Cost: 0.6715804648595336\n",
      "Epoch: 286, Cost: 0.6715972689330183\n",
      "Epoch: 287, Cost: 0.6715680585917947\n",
      "Epoch: 288, Cost: 0.6715872316969079\n",
      "Epoch: 289, Cost: 0.6716241196191284\n",
      "Epoch: 290, Cost: 0.6715795606164274\n",
      "Epoch: 291, Cost: 0.6715706247211101\n",
      "Epoch: 292, Cost: 0.6716668038648399\n",
      "Epoch: 293, Cost: 0.6716174916870186\n",
      "Epoch: 294, Cost: 0.6715762218894806\n",
      "Epoch: 295, Cost: 0.6716253604652674\n",
      "Epoch: 296, Cost: 0.6715845184595364\n",
      "Epoch: 297, Cost: 0.6715671087367948\n",
      "Epoch: 298, Cost: 0.6715975717642898\n",
      "Epoch: 299, Cost: 0.6716363443477378\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.59109889, -0.01578179, -0.0725431 ,  0.1062567 , -0.18950367,\n",
       "       -0.3376476 , -0.13491071, -0.54030759, -0.45237247,  0.0815235 ,\n",
       "        0.29541645, -0.27135088,  0.36880911, -0.04822   , -0.22322151,\n",
       "       -0.34611111,  0.15298622,  0.04611128,  0.04918297, -0.26603479,\n",
       "        0.21901128, -0.23313296,  0.04323745, -0.13930529,  0.19338318,\n",
       "       -0.15718686, -0.20866278, -0.09861212,  0.4208175 ,  0.13223673,\n",
       "        0.13862056,  0.15026483,  0.16059871,  0.06046491,  0.14983613,\n",
       "        0.10738614,  0.23299906,  0.0291932 , -0.59955402, -0.12084722])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 定义逻辑斯蒂克（logistics）目标函数\n",
    "def logistics_function(X, y, weights):\n",
    "    z = np.dot(X, weights)\n",
    "    predictions = 1 / (1 + np.exp(-z))\n",
    "    loss = -np.mean(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))\n",
    "    return loss\n",
    "\n",
    "# 计算逻辑斯蒂克（logistics）目标函数关于参数的梯度\n",
    "def gradient(X, y, weights):\n",
    "    z = np.dot(X, weights)\n",
    "    predictions = 1 / (1 + np.exp(-z))\n",
    "    error = predictions - y\n",
    "    grad = np.dot(X.T, error) / len(X)\n",
    "    return grad\n",
    "\n",
    "# 小批量梯度下降算法\n",
    "def mini_batch_gradient_descent(X, y, learning_rate=0.01, batch_size=32, num_epochs=100):\n",
    "    # 初始化参数\n",
    "    # weights = np.random.randn(X.shape[1])\n",
    "    weights = standard_weight\n",
    "    \n",
    "    # 迭代训练\n",
    "    for epoch in range(num_epochs):\n",
    "        # 在每个epoch开始时，对训练数据进行洗牌\n",
    "        shuffled_indices = np.random.permutation(len(X))\n",
    "        X_shuffled = X[shuffled_indices]\n",
    "        y_shuffled = y[shuffled_indices]\n",
    "        \n",
    "        # 按批次更新参数\n",
    "        for batch_start in range(0, len(X), batch_size):\n",
    "            # 获取当前批次的数据\n",
    "            X_batch = X_shuffled[batch_start:batch_start+batch_size]\n",
    "            y_batch = y_shuffled[batch_start:batch_start+batch_size]\n",
    "            \n",
    "            # 计算梯度\n",
    "            grad = gradient(X_batch, y_batch, weights)\n",
    "            \n",
    "            # 更新参数\n",
    "            weights -= learning_rate * grad\n",
    "            \n",
    "        # 计算当前参数下的目标函数值\n",
    "        cost = logistics_function(X, y, weights)\n",
    "        \n",
    "        # 输出损失\n",
    "        print(\"Epoch: {}, Cost: {}\".format(epoch, cost))\n",
    "    return weights\n",
    "    \n",
    "\n",
    "# # 创建训练数据（示例数据，自行替换）\n",
    "# np.random.seed(0)\n",
    "# X = np.random.randn(1000, 3)\n",
    "# y = np.random.randint(2, size=1000)\n",
    "\n",
    "# # 添加偏置项\n",
    "# X_bias = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "# 运行小批量梯度下降算法\n",
    "mini_batch_gradient_descent(X_bias, y, learning_rate=0.01, batch_size=32, num_epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
